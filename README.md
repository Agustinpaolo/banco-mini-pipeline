# Banco Mini Pipeline

Proyecto personal de aprendizaje para practicar conceptos b√°sicos de ingenier√≠a de datos:

- Orquestaci√≥n de un pipeline ETL con **Apache Airflow**.
- Transformaci√≥n de datos en **Python ‚Äúcl√°sico‚Äù** (m√≥dulo ETL simple).
- Transformaci√≥n equivalente usando **PySpark** como motor distribuido.
- Uso de un dataset de transacciones bancarias ficticias en formato CSV.

El objetivo es tener un proyecto peque√±o pero completo, que pueda mostrarse en un portfolio como ejemplo de pipeline de datos de punta a punta.

---

## Puntos clave del proyecto

- üß† **Separaci√≥n de responsabilidades**  
  El DAG de Airflow solo orquesta; la l√≥gica de negocio vive en m√≥dulos Python externos.

- üêç **ETL simple en Python**  
  Lectura de CSV, agregaci√≥n por cliente (`Sender Account ID`) y escritura de un resumen procesado.

- ‚ö° **Versi√≥n equivalente en PySpark**  
  Se replica la l√≥gica del ETL simple usando Spark, con esquema inferido y funciones de agregaci√≥n distribuidas.

- üìÇ **Estructura clara de datos**  
  Datos crudos en `data/raw/` y resultados en `data/processed/`.

- üß™ **Dataset peque√±o incluido en el repo**  
  El CSV pesa ~130 KB, ideal para pruebas r√°pidas sin depender de fuentes externas.

---

## Arquitectura general

### 1. Airflow + ETL en Python

- El DAG principal se define en `dags/banco_etl_dag.py` y crea un DAG llamado `banco_transactions_etl`.  
- Este DAG llama a la funci√≥n `etl_transactions` definida en `dags/etl_transactions.py`.  
- La funci√≥n:
  - Lee el archivo CSV de transacciones crudo.
  - Agrega montos y cantidades por `Sender Account ID`.
  - Genera un CSV procesado con m√©tricas por cliente.

Dentro del contenedor de Airflow, el c√≥digo espera encontrar:

- Entrada: `/opt/airflow/data/raw/transactions.csv`  
- Salida: `/opt/airflow/data/processed/sender_debits_summary.csv`

El mapeo de vol√∫menes (host ‚Üí contenedor) se configura en `docker-compose.yaml`.

### 2. ETL con PySpark

- El script de Spark est√° en `spark/spark_transform.py`.
- Se ejecuta de forma independiente (no desde Airflow).
- Hace, a grandes rasgos, lo mismo que el ETL simple:
  - Lee `data/raw/transactions.csv` desde el sistema de archivos local.
  - Normaliza `Transaction Type` (min√∫sculas, sin espacios).
  - Filtra solo transacciones de d√©bito (`withdrawal`, `transfer`).
  - Convierte `Transaction Amount` a num√©rico y descarta montos inv√°lidos.
  - Calcula:
    - `total_debit`
    - `avg_debit`
    - `debit_count`
  - Escribe el resultado en `data/processed/sender_debits_summary_spark.csv`.

---

## Dataset de transacciones

El proyecto utiliza un archivo CSV con transacciones bancarias ficticias:

- Ruta (host): `data/raw/transactions.csv`
- Tama√±o aproximado: ~130 KB
- Columnas:

```text
Transaction ID,
Sender Account ID,
Receiver Account ID,
Transaction Amount,
Transaction Type,
Timestamp,
Transaction Status,
Fraud Flag,
Geolocation (Latitude/Longitude),
Device Used,
Network Slice ID,
Latency (ms),
Slice Bandwidth (Mbps),
PIN Code
```

Las transformaciones se centran principalmente en:

- `Sender Account ID`
- `Transaction Amount`
- `Transaction Type`

---

## Estructura del repositorio

Estructura aproximada:

```text
banco-mini-pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ banco_etl_dag.py          # DAG de Airflow
‚îÇ   ‚îî‚îÄ‚îÄ etl_transactions.py       # ETL simple en Python
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îî‚îÄ‚îÄ spark_transform.py        # ETL equivalente en PySpark
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transactions.csv      # Dataset de entrada (incluido en el repo)
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ sender_debits_summary.csv
‚îÇ       ‚îî‚îÄ‚îÄ sender_debits_summary_spark.csv
‚îú‚îÄ‚îÄ docker-compose.yaml           # Orquestaci√≥n de Airflow con Docker
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias para el script de Spark (pyspark)
‚îî‚îÄ‚îÄ README.md
```

> Nota: la carpeta `data/processed/` se genera al ejecutar los ETL (Python y Spark).

---

## Requisitos previos

- **Docker Desktop** (o Docker Engine) instalado y funcionando.
- **Docker Compose** (en versiones nuevas ya viene como `docker compose`).
- **Python 3.10+** (para ejecutar el ETL de Spark desde el host).
- **Java** (JDK o JRE) instalado y accesible si PySpark lo requiere en tu entorno local.

---

## Puesta en marcha r√°pida

### 1. Clonar el repositorio

```bash
git clone https://github.com/<tu-usuario>/banco-mini-pipeline.git
cd banco-mini-pipeline
```

(Ajust√° la URL seg√∫n tu GitHub real.)

---

## Ejecutar el ETL con Airflow (Docker Compose)

1. Asegurate de tener **Docker Desktop** levantado.
2. Desde la ra√≠z del proyecto, levant√° Airflow con Docker Compose:

```bash
docker compose up -d
```

> Tambi√©n pod√©s hacerlo desde la GUI de Docker Desktop si prefer√≠s, pero el README documenta la variante por l√≠nea de comando.

3. Verific√° que los contenedores se est√©n ejecutando:

```bash
docker compose ps
```

4. Asegurate de que el archivo `data/raw/transactions.csv` exista en el host.  
   El `docker-compose.yaml` debe montar la carpeta local `./data` dentro del contenedor de Airflow (por ejemplo en `/opt/airflow/data/`).

5. Entr√° a la interfaz web de Airflow (puerto definido en `docker-compose.yaml`, t√≠picamente 8080 si usaste la plantilla oficial).

6. En la UI de Airflow:

   - Localiz√° el DAG `banco_transactions_etl`.
   - Activ√° el DAG si est√° pausado.
   - Lanz√° una ejecuci√≥n manual (‚ÄúTrigger DAG‚Äù).

7. Una vez terminada la ejecuci√≥n, deber√≠as ver un archivo similar a:

```text
data/processed/sender_debits_summary.csv
```

con columnas como:

```text
Sender Account ID,total_amount,avg_amount,transaction_count
```

---

## Ejecutar el ETL con PySpark

El ETL de Spark se ejecuta directamente desde el host, fuera de Airflow.

### 1. Crear y activar un entorno virtual (opcional pero recomendado)

```bash
python -m venv .venv
source .venv/bin/activate    # En Windows: .venv\Scriptsactivate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

> Actualmente `requirements.txt` contiene `pyspark`, suficiente para este script.

### 3. Ejecutar el script de Spark

Desde la ra√≠z del proyecto:

```bash
python spark/spark_transform.py
```

Esto:

- Lee `data/raw/transactions.csv`.
- Ejecuta la transformaci√≥n con PySpark.
- Genera:

```text
data/processed/sender_debits_summary_spark.csv
```

---

## Comparaci√≥n entre ETL simple y ETL en Spark

Ambos pipelines producen un resumen por `Sender Account ID` con m√©tricas muy similares, pero:

- El ETL simple en Python:
  - Es m√°s directo y f√°cil de leer.
  - Ideal para datasets peque√±os y para explicar la l√≥gica paso a paso.

- El ETL en PySpark:
  - Escala mejor a vol√∫menes grandes de datos.
  - Conocimiento b√°sico de Spark:
    - lectura de CSV con esquema inferido,
    - uso de `withColumn`, `filter`, `groupBy` y funciones de agregaci√≥n,
    - conversi√≥n del resultado a pandas solo al final (por simplicidad en este proyecto).

---

## Ideas de mejora (trabajo futuro)

Algunas extensiones posibles para seguir aprendiendo:

- A√±adir tests unitarios para la l√≥gica de agregaci√≥n.
- Parametrizar rutas de entrada/salida v√≠a variables de entorno o `airflow.Variable`.

---

## Estado del proyecto

Proyecto en desarrollo como parte de mi camino de aprendizaje hacia roles de **Data Engineer Jr.** y proyectos de datos m√°s complejos.  
El objetivo principal es demostrar comprensi√≥n de:

- Orquestaci√≥n de ETLs con Airflow.
- Transformaciones b√°sicas con Python.
- Uso inicial de PySpark en un flujo reproducible.
